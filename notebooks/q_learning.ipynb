{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b3e3e73-6f36-4f64-b8bd-851314824649",
   "metadata": {},
   "source": [
    "<h1>Q-Learning: From Theory to Implementation.</h1>\n",
    "\n",
    "In this notebook we introduce the fundamental Q-learning algorithm, one of the core methods in Reinforcement Learning (RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a2eb7f8-f9f4-404c-bb16-92d8c757c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f1b3d34-9660-425c-99e5-101c53e3fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (size-1, size-1)\n",
    "        self.state = None\n",
    "        \n",
    "        self.action_space = [0, 1, 2, 3]\n",
    "        self.n_actions = len(self.action_space)\n",
    "\n",
    "    def to_index(self, state):\n",
    "        x, y = state\n",
    "        return x * self.size + y\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start\n",
    "        return self.to_index(self.state)\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "    \n",
    "        if action == 0:  # up\n",
    "            x = max(0, x - 1)\n",
    "        elif action == 1:  # down\n",
    "            x = min(self.size - 1, x + 1)\n",
    "        elif action == 2:  # left\n",
    "            y = max(0, y - 1)\n",
    "        elif action == 3:  # right\n",
    "            y = min(self.size - 1, y + 1)\n",
    "    \n",
    "        new_state = (x, y)\n",
    "        self.state = new_state\n",
    "        state_index = self.to_index(new_state)\n",
    "    \n",
    "        terminated = (new_state == self.goal)\n",
    "        reward = 1 if terminated else 0\n",
    "        truncated = False\n",
    "    \n",
    "        return state_index, reward, terminated, truncated\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.zeros((self.size, self.size), dtype=str)\n",
    "        grid[:] = \".\"\n",
    "        \n",
    "        x, y = self.state\n",
    "        gx, gy = self.goal\n",
    "        \n",
    "        grid[x, y] = \"A\"\n",
    "        grid[gx, gy] = \"G\"\n",
    "        \n",
    "        print(\"\\n\".join(\" \".join(row) for row in grid))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df0f6805-ed3b-4d28-831e-7c8161c210db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . G\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld()\n",
    "state = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7981aae0-76d9-4529-80d6-50edb8101cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_q_table(state_space, action_space):\n",
    "  Qtable = np.zeros((state_space, action_space))\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6f894fb-c882-4f0e-bd17-3d07f009df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = env.size * env.size\n",
    "action_space = env.n_actions\n",
    "\n",
    "Qtable_gridworld = initialize_q_table(state_space, action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1d8cc33-3772-40ff-b6be-548b4b7d1364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "  action = np.argmax(Qtable[state][:])\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69d13a52-babc-4943-bccc-e2fdc657c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "  random_num = random.uniform(0,1)\n",
    "  if random_num > epsilon:\n",
    "    action = greedy_policy(Qtable, state)\n",
    "  else:\n",
    "    action = random.choice(env.action_space)\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc07d1ac-86a6-4e26-8bac-b7e98fd1ba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 5000  \n",
    "learning_rate = 0.7       \n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100       \n",
    "\n",
    "# Environment parameters   \n",
    "max_steps = 99               \n",
    "gamma = 0.95                 \n",
    "eval_seed = []               \n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0            \n",
    "min_epsilon = 0.05           \n",
    "decay_rate = 0.0005        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "090ad30b-4db9-455e-8f35-8a6c601ad0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "  for episode in tqdm(range(n_training_episodes)):\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    for step in range(max_steps):\n",
    "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "\n",
    "      new_state_index, reward, terminated, truncated = env.step(action)\n",
    "\n",
    "      Qtable[state][action] += learning_rate * (reward + gamma * np.max(Qtable[new_state_index]) - Qtable[state][action])\n",
    "\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "\n",
    "      state = new_state_index\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c4da8a2-273c-4417-b7b9-fbf65ced3237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 3978.22it/s]\n"
     ]
    }
   ],
   "source": [
    "Qtable_gridworld = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_gridworld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73232461-c3c1-4d11-ac27-6361eb02f332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66342043, 0.6983373 , 0.66342043, 0.6983373 ],\n",
       "       [0.6983373 , 0.73509189, 0.66342043, 0.73509189],\n",
       "       [0.73509189, 0.77378094, 0.6983373 , 0.77378094],\n",
       "       [0.77378094, 0.81450625, 0.73509189, 0.81450625],\n",
       "       [0.81450625, 0.857375  , 0.77378094, 0.81450625],\n",
       "       [0.66342043, 0.73509189, 0.6983373 , 0.73509189],\n",
       "       [0.6983373 , 0.77378094, 0.6983373 , 0.77378094],\n",
       "       [0.73509189, 0.81450625, 0.73509189, 0.81450625],\n",
       "       [0.77378094, 0.857375  , 0.77378094, 0.857375  ],\n",
       "       [0.81450625, 0.9025    , 0.81450625, 0.857375  ],\n",
       "       [0.6983373 , 0.77378094, 0.73509189, 0.77378094],\n",
       "       [0.73509189, 0.81450625, 0.73509189, 0.81450625],\n",
       "       [0.77378094, 0.857375  , 0.77378094, 0.857375  ],\n",
       "       [0.81450625, 0.9025    , 0.81450625, 0.9025    ],\n",
       "       [0.857375  , 0.95      , 0.857375  , 0.9025    ],\n",
       "       [0.73509189, 0.81450625, 0.77378094, 0.81450625],\n",
       "       [0.77378094, 0.857375  , 0.77378094, 0.857375  ],\n",
       "       [0.81450625, 0.9025    , 0.81450625, 0.9025    ],\n",
       "       [0.857375  , 0.95      , 0.857375  , 0.95      ],\n",
       "       [0.9025    , 1.        , 0.9025    , 0.95      ],\n",
       "       [0.77378094, 0.81450625, 0.81450625, 0.857375  ],\n",
       "       [0.81450625, 0.857375  , 0.81450625, 0.9025    ],\n",
       "       [0.857375  , 0.9025    , 0.857375  , 0.95      ],\n",
       "       [0.9025    , 0.95      , 0.9025    , 1.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qtable_gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab7a9107-19ce-48bd-805f-534d95ac8654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
    "  \n",
    "  episode_rewards = []\n",
    "  for episode in tqdm(range(n_eval_episodes)):\n",
    "    if seed:\n",
    "      state = env.reset(seed=seed[episode])\n",
    "    else:\n",
    "      state = env.reset()\n",
    "    step = 0\n",
    "    truncated = False\n",
    "    terminated = False\n",
    "    total_rewards_ep = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "      action = greedy_policy(Q, state)\n",
    "      new_state, reward, terminated, truncated = env.step(action)\n",
    "      total_rewards_ep += reward\n",
    "\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "      state = new_state\n",
    "    episode_rewards.append(total_rewards_ep)\n",
    "  mean_reward = np.mean(episode_rewards)\n",
    "  std_reward = np.std(episode_rewards)\n",
    "\n",
    "  return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5fc9e69-7419-4cb3-9d8b-e99a9244e082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 8438.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_reward=1.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our Agent\n",
    "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_gridworld, eval_seed)\n",
    "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
